# -*- coding: utf-8 -*-
"""~p. 285 - Random Forest, GradientBoosting.ipynb

Automatically generated by Colaboratory.
"""

# 트리의 앙상블

# 정형데이터: CSV와 같이 정형적인 데이터를 뜻한다.
# 비정형데이터: 텍스트 데이터, 사진, 음악 등.

# 정형 데이터에 가장 잘 맞는 알고리즘은 앙상블 학습이고, 이 알고리즘은 결정 트리를 기반으로 만들어져 있다.
# 비정형 데이터는 신경망 학습을 통해서 해결해야 한다.

# 랜덤 포레스트 - 정형 데이터의 끝판왕. 가장 먼저 시도해 봐야 할 것.

# 이 알고리즘은 훈련 세트에서 추출할 때, 이미 뽑았던 세트를 다시 집어넣고 랜덤하게 뽑는다.
# 즉, 중복이 가능하다. 이를 부트스트랩 샘플이라고 부른다.

# 우리가 입력한 훈련 세트와 부트스트랩 샘플의 수는 같아야 한다.

# 트리에서 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 최선의 분할을 찾는다.
# 이때 개수는 전체 특성 개수의 제곱근으로 그 수를 선택한다.

# 기본적으로 100개의 트리를 학습해서 그 평균에서 가장 높은 확률을 가진 클래스로 예측한다.

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
wine = pd.read_csv("https://bit.ly/wine_csv_data")
data = wine[['alcohol','sugar','pH']].to_numpy()
target = wine['class'].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
# 여기까지는 똑같음.

from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
# 여기서 oob_score을 설정하면 부트스트랩 샘플을 만들때 사용하지 않은 샘플들을 사용하여 검증세트처럼 사용한 점수를 얻는다.
rf = RandomForestClassifier(n_jobs=-1, random_state=42,oob_score=True)
# cross_validate에서 return_train_score을 키면 훈련세트에 대한 점수까지 얻을 수 있다.
# 과대적합 여부를 쉽게 알아볼 수 있다.
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
#print(np.mean(scores['train_score']), np.mean(scores['test_score']))
rf.fit(train_input,train_target)
print(rf.oob_score_)
# 교차검증과 비슷한 점수를 얻는다 -> 교차검증을 안할 수 있어서 훈련세트에 더 많은 샘플을 사용가능하다.

# --------------------------------------------
# 엑스트라 트리
# 랜덤 포레스트와 비슷하지만 부트스트랩 샘플을 사용하지 않고 전체 훈련세트를 사용하여 트리를 만든다.
# 단, 노드를 분할할 때 가장 좋은 분할을 찾는게 아니고 무작위로 분할한다.
# 특성을 무작위로 분할하면 안 좋을 것 같은데 트리를 많이 만들기 때문에 과대적합을 막고 검증 세트의 점수를 높인다.
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 결정트리는 최적의 노드 분할을 찾는데 시간이 오래 걸리는데 이렇게 랜덤으로 자르면 쉽게 찾는다.

# --------------------------------------------
# 그레이디언트 부스팅
# 깊이가 얕은 결정 트리를 통해 오차를 보완하는 방식
# 기본적으로 깊이가 3인 결정트리를 100개 사용.
from sklearn.ensemble import GradientBoostingClassifier
# 학습률을 높여보자.
gb = GradientBoostingClassifier(n_estimators=500,learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# --------------------------------------------
# 히스토그램 기반 그레이디언트 부스팅
# 가장 인기가 좋은 정형데이터 머신러닝 알고리즘
# 입력된 특성을 256등분하여 사용하고, 한 등분은 남겨둔다. 누락된 특성을 위해서.
from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

from sklearn.inspection import permutation_importance
hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)
print(result.importances_mean)

hgb.score(test_input,test_target)

# 위 hgb를 구현한 다른 라이브러리
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores=cross_validate(xgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

# 이외에도 LGBM등이 있다. (MS에서 만듦, 사이킷런이 여기서 영향을 많이 받음.)

