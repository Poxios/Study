{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "~p.525 RNN - word embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH8EohaehQ9H",
        "outputId": "d8003b5f-c950-4df0-8686-d62d4ed1d91b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "(25000,) (25000,)\n"
          ]
        }
      ],
      "source": [
        "# 순환 신경망으로 IMDB 분류하기\n",
        "\n",
        "# 긍정, 부정 리뷰를 분류해보자.\n",
        "\n",
        "# 영어단어를 통째로 모델에 넣진 않는다. 고유한 단어 단위로 정수를 부여해서\n",
        "# 그 정수를 넣는다. 이 때, 대문자는 다 소문자로 바꾸고 그 단위는 토큰이라고 부른다.\n",
        "\n",
        "# 예약된 것들도 있다.\n",
        "# 0 - 패딩 ()\n",
        "# 1 - 문장의 시작\n",
        "# 2 - 어휘 사전엔 없는 토큰\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)\n",
        "print(train_input.shape, test_input.shape)\n",
        "# 크기가 1차원 배열인 이유는 샘플의 문장 길이가 다 다르기 때문에 파이썬 리스트에 담아야\n",
        "# 메모리를 효율적으로 사용할 수 있기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_input[0]))\n",
        "print(train_input[0])\n",
        "print(train_target[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIGpc1r6k2CQ",
        "outputId": "8c2041d0-a7d9-4d75-d186-3e3a43cb8b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218\n",
            "[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_input,val_input,train_target,val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)\n",
        "# 훈련세트에서 검증세트를 뽑아보자."
      ],
      "metadata": {
        "id": "md-qGR3MlSbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "lengths = np.array([len(x) for x in train_input])\n",
        "print(np.mean(lengths), np.median(lengths))\n",
        "# 평균은 239인데 중간값은 178인걸로 보아 이것은 치우친 데이터인것 같다!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNukT8zAnxrS",
        "outputId": "e0c16b11-f6cf-4c37-c59c-ac0e9d178850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "239.00925 178.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(lengths)\n",
        "plt.xlabel('length')\n",
        "plt.ylabel('frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XWzFMWeyn-3B",
        "outputId": "22d480c6-dc20-45f3-8434-42d2318793a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "train_seq = pad_sequences(train_input, maxlen = 100)\n",
        "\n",
        "print(train_seq.shape)\n",
        "print(train_seq[0])\n",
        "\n",
        "print(train_input[0][-10:])\n",
        "\n",
        "# 자 그렇다면, pad_sequences는 문장의 앞 부분을 잘라내는 건지 \n",
        "# 뒷 부분을 잘라내는건지 알 수 있다.\n",
        "# 음수 인덱싱을 통해 비교해보면 앞이 날아갔다.\n",
        "\n",
        "# 뒷부분을 자르고 싶으면 pad_sequences() 함수의 truncating 매개변수의 값을 pre -> post로 바꾸면 됨.\n",
        "# 같은 이유로 0 패딩은 앞에 붙는다.\n",
        "\n",
        "# 검증 세트도 동일하게 맞춘다.\n",
        "val_seq = pad_sequences(val_input, maxlen=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffvwb5guo055",
        "outputId": "a350232b-5624-493b-f6c4-5a4c6d8527f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100)\n",
            "[ 10   4  20   9   2 364 352   5  45   6   2   2  33 269   8   2 142   2\n",
            "   5   2  17  73  17 204   5   2  19  55   2   2  92  66 104  14  20  93\n",
            "  76   2 151  33   4  58  12 188   2 151  12 215  69 224 142  73 237   6\n",
            "   2   7   2   2 188   2 103  14  31  10  10 451   7   2   5   2  80  91\n",
            "   2  30   2  34  14  20 151  50  26 131  49   2  84  46  50  37  80  79\n",
            "   6   2  46   7  14  20  10  10 470 158]\n",
            "[6, 2, 46, 7, 14, 20, 10, 10, 470, 158]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 케라스는 여러 순환층 클래스를 제공하는데, 그 중 SimpleRNN이 가장 간단하다.\n",
        "from tensorflow import keras\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.SimpleRNN(8, input_shape=(100,500))) # 500은 어디서 왔을까?\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 갑자기 SimpleRNN에 500은 어디서 왔는가?\n",
        "# 처음 입력에서 단어 입력을 정수로 했는데,\n",
        "# 이것의 문제는 사실 10, 20의 정수 입력은 값이 큰데에 전혀 의미가 없는데\n",
        "# 그 의미가 반영된다는 것이다.\n",
        "# 따라서 원-핫 코딩을 해야한다. [0,0,0,0,0,1,0,0] 이런 식으로.\n",
        "\n",
        "train_oh = keras.utils.to_categorical(train_seq)\n",
        "print(train_oh.shape) # 변환 완료\n",
        "val_oh = keras.utils.to_categorical(val_seq)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU8mhcJprUrp",
        "outputId": "0494addf-2445-404e-a631-056be5037cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100, 500)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 8)                 4072      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,081\n",
            "Trainable params: 4,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5', save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\n",
        "history = model.fit(train_oh, train_target, epochs=100, batch_size=64, validation_data=(val_oh, val_target),callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGwgqHwBvo04",
        "outputId": "1f2b1a85-0ad9-485f-e861-3aaca55b63d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 40s 112ms/step - loss: 0.7000 - accuracy: 0.4954 - val_loss: 0.6991 - val_accuracy: 0.4914\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 33s 106ms/step - loss: 0.6970 - accuracy: 0.5033 - val_loss: 0.6969 - val_accuracy: 0.4974\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 35s 113ms/step - loss: 0.6946 - accuracy: 0.5107 - val_loss: 0.6952 - val_accuracy: 0.5052\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 33s 106ms/step - loss: 0.6926 - accuracy: 0.5188 - val_loss: 0.6937 - val_accuracy: 0.5090\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 34s 109ms/step - loss: 0.6908 - accuracy: 0.5281 - val_loss: 0.6925 - val_accuracy: 0.5126\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 34s 109ms/step - loss: 0.6891 - accuracy: 0.5379 - val_loss: 0.6913 - val_accuracy: 0.5192\n",
            "Epoch 7/100\n",
            "166/313 [==============>...............] - ETA: 15s - loss: 0.6872 - accuracy: 0.5440"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train','val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lop-nAtQwqRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 근데 생각을 해보자. 원-핫 코딩을 계속 사용하면 용량이 미친듯이 커진다.\n",
        "# 그래서 쓰는게 단어 임베딩이다.\n",
        "\n",
        "# 단어 임베딩은 각 단어를 고정된 크기의 실수 벡터로 바꾸어 줍니다.\n",
        "\n",
        "model2 = keras.Sequential()\n",
        "model2.add(keras.layers.Embedding(500,16,input_length=100))\n",
        "model2.add(keras.layers.SimpleRNN(8))\n",
        "model2.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "acKA2fH2xDoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5', save_best_only=True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\n",
        "history = model2.fit(train_oh, train_target, epochs=100, batch_size=64, validation_data=(val_oh, val_target),callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "metadata": {
        "id": "-mWNuM_oaQc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train','val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bQcGnbCGaoYq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}